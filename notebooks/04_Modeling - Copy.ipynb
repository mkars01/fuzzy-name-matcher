{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ad0746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "import tensorflow as tf \n",
    "import random\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "# from tensorflow.python.platform import gfile\n",
    "from tensorflow.io import gfile\n",
    "# import tensorflow.compat.v1.gfile as gfile\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b84edf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interim Data File Locations\n",
    "interim_data = '../data/interim/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05000356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87550, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(interim_data + 'feature_engineering_results.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89b085e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>target</th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>partial</th>\n",
       "      <th>tkn_sort</th>\n",
       "      <th>tkn_set</th>\n",
       "      <th>sum_ipa</th>\n",
       "      <th>pshp_soundex_first</th>\n",
       "      <th>...</th>\n",
       "      <th>editex</th>\n",
       "      <th>saps</th>\n",
       "      <th>flexmetric</th>\n",
       "      <th>jaro</th>\n",
       "      <th>higueramico</th>\n",
       "      <th>sift4</th>\n",
       "      <th>eudex</th>\n",
       "      <th>aline</th>\n",
       "      <th>covington</th>\n",
       "      <th>phoneticeditdistance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46082</th>\n",
       "      <td>Roge</td>\n",
       "      <td>Care</td>\n",
       "      <td>0</td>\n",
       "      <td>roge</td>\n",
       "      <td>care</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0.407258</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.27500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19601</th>\n",
       "      <td>Bety</td>\n",
       "      <td>Wole</td>\n",
       "      <td>0</td>\n",
       "      <td>bety</td>\n",
       "      <td>wole</td>\n",
       "      <td>67</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.22500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.868137</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.443182</td>\n",
       "      <td>0.850806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53813</th>\n",
       "      <td>Parmelia</td>\n",
       "      <td>Field</td>\n",
       "      <td>0</td>\n",
       "      <td>parmelia</td>\n",
       "      <td>field</td>\n",
       "      <td>44</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.728111</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.26875</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.780882</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.538306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20165</th>\n",
       "      <td>Patricia</td>\n",
       "      <td>Ricardo</td>\n",
       "      <td>0</td>\n",
       "      <td>patricia</td>\n",
       "      <td>ricardo</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.31250</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.270960</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.699510</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.675403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19890</th>\n",
       "      <td>Onne</td>\n",
       "      <td>Fredricka</td>\n",
       "      <td>0</td>\n",
       "      <td>onne</td>\n",
       "      <td>fredricka</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>0.641129</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.453704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.557843</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72047</th>\n",
       "      <td>Jen</td>\n",
       "      <td>Francois</td>\n",
       "      <td>0</td>\n",
       "      <td>jen</td>\n",
       "      <td>francois</td>\n",
       "      <td>57</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.858871</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.240909</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.330645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85441</th>\n",
       "      <td>Reg</td>\n",
       "      <td>Rich</td>\n",
       "      <td>0</td>\n",
       "      <td>reg</td>\n",
       "      <td>rich</td>\n",
       "      <td>57</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.42500</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.995098</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.725806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78821</th>\n",
       "      <td>Phil</td>\n",
       "      <td>Lum</td>\n",
       "      <td>0</td>\n",
       "      <td>phil</td>\n",
       "      <td>lum</td>\n",
       "      <td>57</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.752688</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.682843</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.596774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75534</th>\n",
       "      <td>Johannah</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>johannah</td>\n",
       "      <td>france</td>\n",
       "      <td>64</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0.453405</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.48750</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.840686</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.637097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68161</th>\n",
       "      <td>Tulla</td>\n",
       "      <td>Slavi</td>\n",
       "      <td>0</td>\n",
       "      <td>tulla</td>\n",
       "      <td>slavi</td>\n",
       "      <td>69</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.32000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.922059</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.546296</td>\n",
       "      <td>0.735484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              a          b  target    name_a     name_b  partial  tkn_sort  \\\n",
       "46082      Roge       Care       0      roge       care       75        40   \n",
       "19601      Bety       Wole       0      bety       wole       67        40   \n",
       "53813  Parmelia      Field       0  parmelia      field       44        27   \n",
       "20165  Patricia    Ricardo       0  patricia    ricardo       63        63   \n",
       "19890      Onne  Fredricka       0      onne  fredricka       50        25   \n",
       "72047       Jen   Francois       0       jen   francois       57        17   \n",
       "85441       Reg       Rich       0       reg       rich       57        29   \n",
       "78821      Phil        Lum       0      phil        lum       57        29   \n",
       "75534  Johannah     France       0  johannah     france       64        24   \n",
       "68161     Tulla      Slavi       0     tulla      slavi       69        50   \n",
       "\n",
       "       tkn_set   sum_ipa  pshp_soundex_first  ...    editex      saps  \\\n",
       "46082       40  0.407258                   0  ...  0.375000  0.000000   \n",
       "19601       40  0.645161                   0  ...  0.250000  0.000000   \n",
       "53813       27  0.728111                   0  ...  0.437500  0.000000   \n",
       "20165       63  0.500000                   0  ...  0.250000  0.130435   \n",
       "19890       25  0.641129                   0  ...  0.166667  0.000000   \n",
       "72047       17  0.858871                   0  ...  0.250000  0.000000   \n",
       "85441       29  0.919355                   1  ...  0.375000  0.071429   \n",
       "78821       29  0.752688                   0  ...  0.125000  0.000000   \n",
       "75534       24  0.453405                   0  ...  0.375000  0.000000   \n",
       "68161       50  0.634409                   0  ...  0.200000  0.000000   \n",
       "\n",
       "       flexmetric      jaro  higueramico     sift4     eudex     aline  \\\n",
       "46082     0.27500  0.500000     0.266667  0.500000  0.682353  0.400000   \n",
       "19601     0.22500  0.000000     0.100000  0.250000  0.868137  0.150000   \n",
       "53813     0.26875  0.550000     0.190476  0.250000  0.780882  0.312500   \n",
       "20165     0.31250  0.523810     0.270960  0.375000  0.699510  0.487500   \n",
       "19890     0.25000  0.453704     0.000000  0.111111  0.557843  0.127451   \n",
       "72047     0.21875  0.486111     0.000000  0.125000  0.814706  0.240909   \n",
       "85441     0.42500  0.527778     0.250000  0.250000  0.995098  0.437500   \n",
       "78821     0.17500  0.000000     0.016667  0.250000  0.682843  0.291667   \n",
       "75534     0.48750  0.527778     0.232143  0.250000  0.840686  0.318182   \n",
       "68161     0.32000  0.466667     0.238095  0.400000  0.922059  0.555556   \n",
       "\n",
       "       covington  phoneticeditdistance  \n",
       "46082   0.647727              0.838710  \n",
       "19601   0.443182              0.850806  \n",
       "53813   0.468254              0.538306  \n",
       "20165   0.620000              0.675403  \n",
       "19890   0.360656              0.419355  \n",
       "72047   0.392157              0.330645  \n",
       "85441   0.600000              0.725806  \n",
       "78821   0.428571              0.596774  \n",
       "75534   0.543478              0.637097  \n",
       "68161   0.546296              0.735484  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5608f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING TESTING TESTING\n",
    "df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e11feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Set:  (100, 27)\n",
      "Label Set:  (100,)\n"
     ]
    }
   ],
   "source": [
    "y = df.target\n",
    "X = df.drop('target', axis=1)\n",
    "print(\"Feature Set: \", X.shape)\n",
    "print(\"Label Set: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7f32448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Feature Set:  (80, 27)\n",
      "Training Label Set:  (80,)\n",
      "Testing Feature Set:  (20, 27)\n",
      "Testing Label Set:  (20,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "print(\"Training Feature Set: \", X_train.shape)\n",
    "print(\"Training Label Set: \", y_train.shape)\n",
    "print(\"Testing Feature Set: \", X_test.shape)\n",
    "print(\"Testing Label Set: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a195e9",
   "metadata": {},
   "source": [
    "### Base-Model 1: Exported TPOT Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53dfd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_1(X_train, y_train, X_test, export=False):\n",
    "    exported_pipeline = make_pipeline(\n",
    "        MaxAbsScaler(),\n",
    "        MinMaxScaler(),\n",
    "        RandomForestClassifier(\n",
    "            bootstrap=False,\n",
    "            criterion=\"gini\",\n",
    "            max_features=0.25,\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=4,\n",
    "            n_estimators=100)\n",
    "    )\n",
    "    exported_pipeline.fit(X_train, y_train)\n",
    "    if export==True:\n",
    "        return exported_pipeline\n",
    "    else:\n",
    "        y_pred = exported_pipeline.predict_proba(X_test)\n",
    "        return [p[1] for p in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382027c",
   "metadata": {},
   "source": [
    "### Base-Model 2: Deep LSTM Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ddc5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow based implementation of deep siamese LSTM network.\n",
    "# Taken from https://github.com/dhwajraj/deep-siamese-text-similarity as of 2020-07-20\n",
    "# and modified to fit hmni prediction pipeline\n",
    "# deep-siamese-text-similarity original copyright:\n",
    "#\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2016 Dhwaj Raj\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SiameseLSTM(object):\n",
    "    \"\"\"\n",
    "    A LSTM based deep Siamese network for text similarity.\n",
    "    Uses an character embedding layer, followed by a biLSTM and Energy Loss layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def BiRNN(self, x, dropout, scope, hidden_units):\n",
    "        n_hidden = hidden_units\n",
    "        n_layers = 3\n",
    "\n",
    "        # Prepare data shape to match `static_rnn` function requirements\n",
    "        x = tf.unstack(tf.transpose(x, perm=[1, 0, 2]))\n",
    "\n",
    "        # Define lstm cells with tensorflow\n",
    "        # Forward direction cell\n",
    "        with tf.name_scope('fw' + scope):\n",
    "            with tf.compat.v1.variable_scope('fw' + scope):\n",
    "                stacked_rnn_fw = []\n",
    "                for _ in range(n_layers):\n",
    "                    fw_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "                    lstm_fw_cell = \\\n",
    "                        tf.compat.v1.nn.rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=dropout)\n",
    "                    stacked_rnn_fw.append(lstm_fw_cell)\n",
    "                lstm_fw_cell_m = \\\n",
    "                    tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "\n",
    "        with tf.name_scope('bw' + scope):\n",
    "            with tf.compat.v1.variable_scope('bw' + scope):\n",
    "                stacked_rnn_bw = []\n",
    "                for _ in range(n_layers):\n",
    "                    bw_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "                    lstm_bw_cell = \\\n",
    "                        tf.compat.v1.nn.rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=dropout)\n",
    "                    stacked_rnn_bw.append(lstm_bw_cell)\n",
    "                lstm_bw_cell_m = \\\n",
    "                    tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n",
    "\n",
    "        # Get lstm cell output\n",
    "        with tf.name_scope('bw' + scope):\n",
    "            with tf.compat.v1.variable_scope('bw' + scope):\n",
    "                (outputs, _, _) = \\\n",
    "                    tf.compat.v1.nn.static_bidirectional_rnn(lstm_fw_cell_m,\n",
    "                                                   lstm_bw_cell_m, x, dtype=tf.float32)\n",
    "        return outputs[-1]\n",
    "\n",
    "    def contrastive_loss(self, y, d, batch_size):\n",
    "        tmp = y * tf.square(d)\n",
    "        tmp2 = (1 - y) * tf.square(tf.maximum(1 - d, 0))\n",
    "        return tf.reduce_sum(tmp + tmp2) / batch_size / 2\n",
    "\n",
    "    def __init__(self, sequence_length, vocab_size, embedding_size, hidden_units, batch_size):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.compat.v1.placeholder(tf.int32, [None, sequence_length], name='input_x1')\n",
    "        self.input_x2 = tf.compat.v1.placeholder(tf.int32, [None, sequence_length], name='input_x2')\n",
    "        self.input_y = tf.compat.v1.placeholder(tf.float32, [None], name='input_y')\n",
    "        self.dropout_keep_prob = tf.compat.v1.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.Variable(tf.compat.v1.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                                 trainable=True, name='W')\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        with tf.name_scope('output'):\n",
    "            self.out1 = self.BiRNN(\n",
    "                self.embedded_chars1,\n",
    "                self.dropout_keep_prob,\n",
    "                'side1',\n",
    "                hidden_units\n",
    "            )\n",
    "            self.out2 = self.BiRNN(\n",
    "                self.embedded_chars2,\n",
    "                self.dropout_keep_prob,\n",
    "                'side2',\n",
    "                hidden_units\n",
    "            )\n",
    "            self.distance = \\\n",
    "                tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.out1, self.out2)), 1, keepdims=True))\n",
    "            self.distance = tf.compat.v1.div(self.distance,\n",
    "                                   tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.out1), 1, keepdims=True)),\n",
    "                                          tf.sqrt(tf.reduce_sum(tf.square(self.out2), 1, keepdims=True))))\n",
    "            self.distance = tf.reshape(self.distance, [-1], name='distance')\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = self.contrastive_loss(self.input_y, self.distance, batch_size)\n",
    "\n",
    "        # Accuracy computation is outside of this class.\n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.temp_sim = tf.subtract(tf.ones_like(self.distance),\n",
    "                                        tf.compat.v1.rint(self.distance), name='temp_sim')  # auto threshold 0.5\n",
    "            correct_predictions = tf.equal(self.temp_sim, self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de3794f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow based implementation of deep siamese LSTM network.\n",
    "# Taken from https://github.com/dhwajraj/deep-siamese-text-similarity as of 2020-07-20\n",
    "# and modified to fit hmni prediction pipeline\n",
    "# deep-siamese-text-similarity original copyright:\n",
    "#\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2016 Dhwaj Raj\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "TOKENIZER_RE = re.compile(r\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", re.UNICODE)\n",
    "\n",
    "\n",
    "def tokenizer(iterator):\n",
    "    \"\"\"Tokenizer generator.\n",
    "    Args:\n",
    "      iterator: Input iterator with strings.\n",
    "    Yields:\n",
    "      array of tokens per each value in the input.\n",
    "    \"\"\"\n",
    "    for value in iterator:\n",
    "        yield TOKENIZER_RE.findall(value)\n",
    "\n",
    "\n",
    "class CategoricalVocabulary(object):\n",
    "    \"\"\"Categorical variables vocabulary class.\n",
    "  Accumulates and provides mapping from classes to indexes.\n",
    "  Can be easily used for words.\n",
    "  \"\"\"\n",
    "    def __init__(self, unknown_token=\"<UNK>\", support_reverse=True):\n",
    "        self._unknown_token = unknown_token\n",
    "        self._mapping = {unknown_token: 0}\n",
    "        self._support_reverse = support_reverse\n",
    "        if support_reverse:\n",
    "            self._reverse_mapping = [unknown_token]\n",
    "        self._freq = collections.defaultdict(int)\n",
    "        self._freeze = False\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns total count of mappings. Including unknown token.\"\"\"\n",
    "        return len(self._mapping)\n",
    "\n",
    "    def freeze(self, freeze=True):\n",
    "        \"\"\"Freezes the vocabulary, after which new words return unknown token id.\n",
    "        Args:\n",
    "          freeze: True to freeze, False to unfreeze.\n",
    "        \"\"\"\n",
    "        self._freeze = freeze\n",
    "\n",
    "    def get(self, category):\n",
    "        \"\"\"Returns word's id in the vocabulary.\n",
    "        If category is new, creates a new id for it.\n",
    "        Args:\n",
    "          category: string or integer to lookup in vocabulary.\n",
    "        Returns:\n",
    "          interger, id in the vocabulary.\n",
    "        \"\"\"\n",
    "        if category not in self._mapping:\n",
    "            if self._freeze:\n",
    "                return 0\n",
    "            self._mapping[category] = len(self._mapping)\n",
    "            if self._support_reverse:\n",
    "                self._reverse_mapping.append(category)\n",
    "        return self._mapping[category]\n",
    "\n",
    "    def add(self, category, count=1):\n",
    "        \"\"\"Adds count of the category to the frequency table.\n",
    "        Args:\n",
    "          category: string or integer, category to add frequency to.\n",
    "          count: optional integer, how many to add.\n",
    "        \"\"\"\n",
    "        category_id = self.get(category)\n",
    "        if category_id <= 0:\n",
    "            return\n",
    "        self._freq[category] += count\n",
    "\n",
    "    def trim(self, min_frequency, max_frequency=-1):\n",
    "        \"\"\"Trims vocabulary for minimum frequency.\n",
    "        Remaps ids from 1..n in sort frequency order.\n",
    "        where n - number of elements left.\n",
    "        Args:\n",
    "          min_frequency: minimum frequency to keep.\n",
    "          max_frequency: optional, maximum frequency to keep.\n",
    "            Useful to remove very frequent categories (like stop words).\n",
    "        \"\"\"\n",
    "        # Sort by alphabet then reversed frequency.\n",
    "        self._freq = sorted(\n",
    "            sorted(\n",
    "                self._freq.items(),\n",
    "                key=lambda x: (isinstance(x[0], str), x[0])),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True)\n",
    "        self._mapping = {self._unknown_token: 0}\n",
    "        if self._support_reverse:\n",
    "            self._reverse_mapping = [self._unknown_token]\n",
    "        idx = 1\n",
    "        for category, count in self._freq:\n",
    "            if 0 < max_frequency <= count:\n",
    "                continue\n",
    "            if count <= min_frequency:\n",
    "                break\n",
    "            self._mapping[category] = idx\n",
    "            idx += 1\n",
    "            if self._support_reverse:\n",
    "                self._reverse_mapping.append(category)\n",
    "        self._freq = dict(self._freq[:idx - 1])\n",
    "\n",
    "    def reverse(self, class_id):\n",
    "        \"\"\"Given class id reverse to original class name.\n",
    "        Args:\n",
    "          class_id: Id of the class.\n",
    "        Returns:\n",
    "          Class name.\n",
    "        Raises:\n",
    "          ValueError: if this vocabulary wasn't initialized with support_reverse.\n",
    "        \"\"\"\n",
    "        if not self._support_reverse:\n",
    "            raise ValueError(\"This vocabulary wasn't initialized with \"\n",
    "                             \"support_reverse to support reverse() function.\")\n",
    "        return self._reverse_mapping[class_id]\n",
    "\n",
    "\n",
    "class VocabularyProcessor(object):\n",
    "    \"\"\"Maps documents to sequences of word ids.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_document_length,\n",
    "                 min_frequency=0,\n",
    "                 vocabulary=None,\n",
    "                 tokenizer_fn=None):\n",
    "        \"\"\"Initializes a VocabularyProcessor instance.\n",
    "        Args:\n",
    "          max_document_length: Maximum length of documents.\n",
    "            if documents are longer, they will be trimmed, if shorter - padded.\n",
    "          min_frequency: Minimum frequency of words in the vocabulary.\n",
    "          vocabulary: CategoricalVocabulary object.\n",
    "        Attributes:\n",
    "          vocabulary_: CategoricalVocabulary object.\n",
    "        \"\"\"\n",
    "        self.max_document_length = max_document_length\n",
    "        self.min_frequency = min_frequency\n",
    "        if vocabulary:\n",
    "            self.vocabulary_ = vocabulary\n",
    "        else:\n",
    "            self.vocabulary_ = CategoricalVocabulary()\n",
    "        if tokenizer_fn:\n",
    "            self._tokenizer = tokenizer_fn\n",
    "        else:\n",
    "            self._tokenizer = tokenizer\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        \"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "        Args:\n",
    "          raw_documents: An iterable which yield either str or unicode.\n",
    "        Returns:\n",
    "          self\n",
    "        \"\"\"\n",
    "        for tokens in self._tokenizer(raw_documents):\n",
    "            for token in tokens:\n",
    "                self.vocabulary_.add(token)\n",
    "        if self.min_frequency > 0:\n",
    "            self.vocabulary_.trim(self.min_frequency)\n",
    "        self.vocabulary_.freeze()\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, raw_documents):\n",
    "        \"\"\"Learn the vocabulary dictionary and return indexies of words.\n",
    "        Args:\n",
    "          raw_documents: An iterable which yield either str or unicode.\n",
    "        Returns:\n",
    "          x: iterable, [n_samples, max_document_length]. Word-id matrix.\n",
    "        \"\"\"\n",
    "        self.fit(raw_documents)\n",
    "        return self.transform(raw_documents)\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to word-id matrix.\n",
    "        Convert words to ids with vocabulary fitted with fit or the one\n",
    "        provided in the constructor.\n",
    "        Args:\n",
    "          raw_documents: An iterable which yield either str or unicode.\n",
    "        Yields:\n",
    "          x: iterable, [n_samples, max_document_length]. Word-id matrix.\n",
    "        \"\"\"\n",
    "        for tokens in self._tokenizer(raw_documents):\n",
    "            word_ids = np.zeros(self.max_document_length, np.int64)\n",
    "            for idx, token in enumerate(tokens):\n",
    "                if idx >= self.max_document_length:\n",
    "                    break\n",
    "                word_ids[idx] = self.vocabulary_.get(token)\n",
    "            yield word_ids\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves vocabulary processor into given file.\n",
    "        Args:\n",
    "          filename: Path to output file.\n",
    "        \"\"\"\n",
    "        with gfile.Open(filename, 'wb') as f:\n",
    "            f.write(pickle.dumps(self))\n",
    "\n",
    "    @classmethod\n",
    "    def restore(cls, filename):\n",
    "        \"\"\"Restores vocabulary processor from given file.\n",
    "        Args:\n",
    "          filename: Path to file to load from.\n",
    "        Returns:\n",
    "          VocabularyProcessor object.\n",
    "        \"\"\"\n",
    "        with gfile.Open(filename, 'rb') as f:\n",
    "            return pickle.loads(f.read())\n",
    "\n",
    "\n",
    "def tokenizer_char(iterator):\n",
    "    for value in iterator:\n",
    "        yield list(value)\n",
    "\n",
    "\n",
    "class MyVocabularyProcessor(VocabularyProcessor):\n",
    "    def __init__(self, max_document_length, min_frequency=0, vocabulary=None):\n",
    "        super().__init__(max_document_length, min_frequency, vocabulary)\n",
    "        sup = super(MyVocabularyProcessor, self)\n",
    "        sup.__init__(max_document_length, min_frequency, vocabulary,\n",
    "                     tokenizer_char)\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to word-id matrix.\n",
    "        Convert words to ids with vocabulary fitted with fit or the one\n",
    "        provided in the constructor.\n",
    "        Args:\n",
    "          raw_documents: An iterable which yield either str or unicode.\n",
    "        Yields:\n",
    "          x: iterable, [n_samples, max_document_length]. Word-id matrix.\n",
    "        \"\"\"\n",
    "        for tokens in self._tokenizer(raw_documents):\n",
    "            word_ids = np.zeros(self.max_document_length, np.int64)\n",
    "            for (idx, token) in enumerate(tokens):\n",
    "                if idx >= self.max_document_length:\n",
    "                    break\n",
    "                word_ids[idx] = self.vocabulary_.get(token)\n",
    "            yield word_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3244a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_2(X_train, y_train, X_test, export=False):\n",
    "    \n",
    "    # Train Model\n",
    "    embedding_dim = 300  # Dimensionality of character embedding\n",
    "    dropout_keep_prob = 0.8  # Dropout keep probability\n",
    "    hidden_units = 50\n",
    "    batch_size = 64\n",
    "    num_epochs = 300  # Number of training epochs\n",
    "    evaluate_every = 1000  # Evaluate model on dev set after this many steps\n",
    "    max_document_length = 15\n",
    "    out_dir = os.getcwd()+'\\\\'  # where to save exported models\n",
    "\n",
    "    inpH = InputHelper()\n",
    "    train_set, dev_set, vocab_processor, sum_no_of_batches = \\\n",
    "        inpH.get_datasets(\n",
    "        X_train[['name_a', 'name_b']],\n",
    "        y_train,\n",
    "        max_document_length=max_document_length,\n",
    "        percent_dev=10,\n",
    "        batch_size=64)\n",
    "\n",
    "\n",
    "    # print('starting graph def')\n",
    "    graph = tf.Graph()\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.compat.v1.Session(config=session_conf)\n",
    "        # print('started session')\n",
    "        with sess.as_default():\n",
    "            siameseModel = SiameseLSTM(\n",
    "                sequence_length=max_document_length,\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_dim,\n",
    "                hidden_units=hidden_units,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            # optimizer = tf.optimizers.Adam(1e-3)\n",
    "            # optimizer = Adam(1e-3)\n",
    "            optimizer = tf.compat.v1.train.AdamOptimizer(1e-3)\n",
    "            # print('initialized siameseModel object')\n",
    "\n",
    "        grads_and_vars = optimizer.compute_gradients(siameseModel.loss)\n",
    "        tr_op_set = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        # print('defined training_ops')\n",
    "        \n",
    "        if export==True:\n",
    "            saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables(), max_to_keep=100)\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, 'vocab'))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        def train_step(x1_batch, x2_batch, y_batch):\n",
    "            # A single training step\n",
    "            if random.random() > 0.5:\n",
    "                feed_dict = {\n",
    "                    siameseModel.input_x1: x1_batch,\n",
    "                    siameseModel.input_x2: x2_batch,\n",
    "                    siameseModel.input_y: y_batch,\n",
    "                    siameseModel.dropout_keep_prob: dropout_keep_prob,\n",
    "                }\n",
    "            else:\n",
    "                feed_dict = {\n",
    "                    siameseModel.input_x1: x2_batch,\n",
    "                    siameseModel.input_x2: x1_batch,\n",
    "                    siameseModel.input_y: y_batch,\n",
    "                    siameseModel.dropout_keep_prob: dropout_keep_prob,\n",
    "                }\n",
    "            (_, step, loss, accuracy, dist, sim) = \\\n",
    "                sess.run([tr_op_set, global_step, siameseModel.loss, siameseModel.accuracy,\n",
    "                          siameseModel.distance, siameseModel.temp_sim], feed_dict)\n",
    "\n",
    "        def dev_step(x1_batch, x2_batch, y_batch):\n",
    "            # A single training step\n",
    "            if random.random() > 0.5:\n",
    "                feed_dict = {\n",
    "                    siameseModel.input_x1: x1_batch,\n",
    "                    siameseModel.input_x2: x2_batch,\n",
    "                    siameseModel.input_y: y_batch,\n",
    "                    siameseModel.dropout_keep_prob: 1.0,\n",
    "                }\n",
    "            else:\n",
    "                feed_dict = {\n",
    "                    siameseModel.input_x1: x2_batch,\n",
    "                    siameseModel.input_x2: x1_batch,\n",
    "                    siameseModel.input_y: y_batch,\n",
    "                    siameseModel.dropout_keep_prob: 1.0,\n",
    "                }\n",
    "            (step, loss, accuracy, sim) = \\\n",
    "                sess.run([global_step, siameseModel.loss, siameseModel.accuracy,\n",
    "                          siameseModel.temp_sim], feed_dict)\n",
    "            return accuracy\n",
    "\n",
    "        # Generate batches\n",
    "        batches = inpH.batch_iter(list(zip(train_set[0], train_set[1],\n",
    "                                           train_set[2])), batch_size, num_epochs)\n",
    "        max_validation_acc = 0.0\n",
    "        for nn in range(sum_no_of_batches * num_epochs):\n",
    "            batch = next(batches)\n",
    "            if len(batch) < 1:\n",
    "                continue\n",
    "            (x1_batch, x2_batch, y_batch) = zip(*batch)\n",
    "            if len(y_batch) < 1:\n",
    "                continue\n",
    "            train_step(x1_batch, x2_batch, y_batch)\n",
    "            current_step = tf.compat.v1.train.global_step(sess, global_step)\n",
    "            sum_acc = 0.0\n",
    "            if current_step % evaluate_every == 0:\n",
    "                dev_batches = inpH.batch_iter(list(zip(dev_set[0], dev_set[1], dev_set[2])), batch_size, 1)\n",
    "                for db in dev_batches:\n",
    "                    if len(db) < 1:\n",
    "                        continue\n",
    "                    (x1_dev_b, x2_dev_b, y_dev_b) = zip(*db)\n",
    "                    if len(y_dev_b) < 1:\n",
    "                        continue\n",
    "                    acc = dev_step(x1_dev_b, x2_dev_b, y_dev_b)\n",
    "                    sum_acc = sum_acc + acc\n",
    "            if sum_acc > max_validation_acc:\n",
    "                max_validation_acc = sum_acc\n",
    "            \n",
    "                if export==True:\n",
    "                    # save model\n",
    "                    saver.save(sess, out_dir, global_step=current_step)\n",
    "                    tf.train.write_graph(sess.graph.as_graph_def(), out_dir, 'siamese_network.pb', as_text=False)\n",
    "                \n",
    "                # print('model {} with sum_accuracy={}'.format(nn, max_validation_acc))     \n",
    "        if export==True:\n",
    "            return\n",
    "        \n",
    "        # RUN OOF INFERENCE\n",
    "        x1_temp= np.asarray(X_test['name_a'].tolist())\n",
    "        x2_temp= np.asarray(X_test['name_b'].tolist())\n",
    "        \n",
    "        x1 = np.asarray(list(vocab_processor.transform(x1_temp)))\n",
    "        x2 = np.asarray(list(vocab_processor.transform(x2_temp)))\n",
    "\n",
    "        (predictions, sim) = sess.run([siameseModel.distance, siameseModel.temp_sim], {\n",
    "                siameseModel.input_x1: x1,\n",
    "                siameseModel.input_x2: x2,\n",
    "                siameseModel.dropout_keep_prob: 1.0,\n",
    "            })\n",
    "                \n",
    "        sim = predictions.tolist()\n",
    "        sim = [1-x for x in sim]\n",
    "        # print(sim)\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7d2a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow based implementation of deep siamese LSTM network.\n",
    "# Taken from https://github.com/dhwajraj/deep-siamese-text-similarity as of 2020-07-20\n",
    "# and modified to fit hmni prediction pipeline\n",
    "# deep-siamese-text-similarity original copyright:\n",
    "#\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2016 Dhwaj Raj\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "class InputHelper(object):\n",
    "    vocab_processor = None\n",
    "\n",
    "    def batch_iter(\n",
    "            self,\n",
    "            data,\n",
    "            batch_size,\n",
    "            num_epochs,\n",
    "            shuffle=True,\n",
    "    ):\n",
    "\n",
    "        # Generates a batch iterator for a dataset.\n",
    "        data = np.asarray(data)\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int(len(data) / batch_size) + 1\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = \\\n",
    "                    np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                yield shuffled_data[start_index:end_index]\n",
    "\n",
    "    # Data Preparation\n",
    "    def get_datasets(\n",
    "            self,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            max_document_length,\n",
    "            percent_dev,\n",
    "            batch_size,\n",
    "    ):\n",
    "        (x1_text, x2_text, y) = \\\n",
    "            np.asarray(X_train.iloc[:, 0].str.lower()), np.asarray(X_train.iloc[:, 1].str.lower()), np.asarray(y_train)\n",
    "\n",
    "        # Build vocabulary\n",
    "        # print('Building vocabulary')\n",
    "        vocab_processor = MyVocabularyProcessor(max_document_length, min_frequency=0)\n",
    "        vocab_processor.fit_transform(np.concatenate((x2_text, x1_text), axis=0))\n",
    "        # print('Length of loaded vocabulary ={}'.format(len(vocab_processor.vocabulary_)))\n",
    "\n",
    "        sum_no_of_batches = 0\n",
    "        x1 = np.asarray(list(vocab_processor.transform(x1_text)))\n",
    "        x2 = np.asarray(list(vocab_processor.transform(x2_text)))\n",
    "\n",
    "        # Randomly shuffle data\n",
    "        np.random.seed(131)\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "        x1_shuffled = x1[shuffle_indices]\n",
    "        x2_shuffled = x2[shuffle_indices]\n",
    "        y_shuffled = y[shuffle_indices]\n",
    "        dev_idx = -1 * len(y_shuffled) * percent_dev // 100\n",
    "        del x1\n",
    "        del x2\n",
    "\n",
    "        # TODO: This is very crude, should use cross-validation\n",
    "        (x1_train, x1_dev) = (x1_shuffled[:dev_idx], x1_shuffled[dev_idx:])\n",
    "        (x2_train, x2_dev) = (x2_shuffled[:dev_idx], x2_shuffled[dev_idx:])\n",
    "        (y_train, y_dev) = (y_shuffled[:dev_idx], y_shuffled[dev_idx:])\n",
    "        # print('Train/Dev split for data: {:d}/{:d}'.format(len(y_train), len(y_dev)))\n",
    "\n",
    "        sum_no_of_batches = sum_no_of_batches + len(y_train) // batch_size\n",
    "        train_set = (x1_train, x2_train, y_train)\n",
    "        dev_set = (x1_dev, x2_dev, y_dev)\n",
    "        gc.collect()\n",
    "        return train_set, dev_set, vocab_processor, sum_no_of_batches\n",
    "\n",
    "    def getTestDataSet(\n",
    "            self,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            vocab,\n",
    "            max_document_length,\n",
    "    ):\n",
    "        (x1_temp, x2_temp, y) = np.asarray(X_test.iloc[:, 0].str.lower()), np.asarray(\n",
    "            X_test.iloc[:, 1].str.lower()), np.asarray(y_test)\n",
    "\n",
    "        # Build vocabulary\n",
    "        vocab_processor = MyVocabularyProcessor(max_document_length, min_frequency=0)\n",
    "        vocab_processor = vocab\n",
    "\n",
    "        x1 = np.asarray(list(vocab_processor.transform(x1_temp)))\n",
    "        x2 = np.asarray(list(vocab_processor.transform(x2_temp)))\n",
    "\n",
    "        # Randomly shuffle data\n",
    "        del vocab_processor\n",
    "        gc.collect()\n",
    "        return x1, x2, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9422f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\1653033814.py:51: LSTMCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\1653033814.py:56: MultiRNNCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\1653033814.py:73: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mkars\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:1579: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\mkars\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:962: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\mkars\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:970: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\1653033814.py:114: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\856590830.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['target'] = y_test.tolist()\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  meta_training = meta_training.append(oof_pred)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\856590830.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['target'] = y_test.tolist()\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  meta_training = meta_training.append(oof_pred)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\856590830.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['target'] = y_test.tolist()\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  meta_training = meta_training.append(oof_pred)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\856590830.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['target'] = y_test.tolist()\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  meta_training = meta_training.append(oof_pred)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\856590830.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['target'] = y_test.tolist()\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  meta_training = meta_training.append(oof_pred)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\856590830.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['target'] = y_test.tolist()\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  meta_training = meta_training.append(oof_pred)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\856590830.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['target'] = y_test.tolist()\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  meta_training = meta_training.append(oof_pred)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\856590830.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['target'] = y_test.tolist()\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  meta_training = meta_training.append(oof_pred)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\856590830.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.asarray(data)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['target'] = y_test.tolist()\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  meta_training = meta_training.append(oof_pred)\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
      "C:\\Users\\mkars\\AppData\\Local\\Temp\\ipykernel_20932\\2741245644.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 9 of 10\n"
     ]
    }
   ],
   "source": [
    "# Stratified K-Folds cross-validator\n",
    "meta_training = pd.DataFrame()\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    oof_pred = X_test[['name_a', 'name_b']]\n",
    "    \n",
    "    oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
    "                                      y_train,\n",
    "                                      X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
    "\n",
    "    oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
    "                                      y_train,\n",
    "                                      X_test[['name_a', 'name_b']])\n",
    "    \n",
    "    oof_pred['target'] = y_test.tolist()\n",
    "    \n",
    "    print('completed fold {} of 10'.format(fold))\n",
    "    fold += 1\n",
    "\n",
    "    meta_training = meta_training.append(oof_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b85afd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>predict_proba</th>\n",
       "      <th>siamese_sim</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44178</th>\n",
       "      <td>jacob</td>\n",
       "      <td>toncka</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.011047</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44402</th>\n",
       "      <td>david</td>\n",
       "      <td>josefina</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>ljuba</td>\n",
       "      <td>ljubisa</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.842422</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61881</th>\n",
       "      <td>augustina</td>\n",
       "      <td>sigridur</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.525438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40396</th>\n",
       "      <td>ruby</td>\n",
       "      <td>wen</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.238432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53560</th>\n",
       "      <td>migue</td>\n",
       "      <td>fran</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008486</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>catherine</td>\n",
       "      <td>katherina</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.960835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59614</th>\n",
       "      <td>ance</td>\n",
       "      <td>titia</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.088846</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8270</th>\n",
       "      <td>kori</td>\n",
       "      <td>chucky</td>\n",
       "      <td>0.043333</td>\n",
       "      <td>0.767349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20973</th>\n",
       "      <td>kori</td>\n",
       "      <td>yarik</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.047350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name_a     name_b  predict_proba  siamese_sim  target\n",
       "44178      jacob     toncka       0.010000     0.011047       0\n",
       "44402      david   josefina       0.000000     0.150563       0\n",
       "9221       ljuba    ljubisa       0.990000     0.842422       1\n",
       "61881  augustina   sigridur       0.021667     0.525438       0\n",
       "40396       ruby        wen       0.060000     0.238432       0\n",
       "53560      migue       fran       0.000000     0.008486       0\n",
       "2465   catherine  katherina       0.973333     0.960835       1\n",
       "59614       ance      titia       0.091667     0.088846       0\n",
       "8270        kori     chucky       0.043333     0.767349       1\n",
       "20973       kori      yarik       0.208333     0.047350       0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_training.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "006e6d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>predict_proba</th>\n",
       "      <th>siamese_sim</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8482</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>ina</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.080234</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8483</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>lavina</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.547983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8485</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>louvinia</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.771870</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8486</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>vina</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.085932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8487</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>vine</td>\n",
       "      <td>0.738333</td>\n",
       "      <td>0.092360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8484</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>louvina</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.826876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8488</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>vinne</td>\n",
       "      <td>0.838333</td>\n",
       "      <td>0.716403</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8489</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>wyncha</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25631</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>jonna</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.073134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55876</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>patricia</td>\n",
       "      <td>0.071667</td>\n",
       "      <td>0.195781</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69592</th>\n",
       "      <td>lavinia</td>\n",
       "      <td>ditus</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name_a    name_b  predict_proba  siamese_sim  target\n",
       "8482   lavinia       ina       0.570000     0.080234       1\n",
       "8483   lavinia    lavina       1.000000     0.547983       1\n",
       "8485   lavinia  louvinia       0.991667     0.771870       1\n",
       "8486   lavinia      vina       0.973333     0.085932       1\n",
       "8487   lavinia      vine       0.738333     0.092360       1\n",
       "8484   lavinia   louvina       0.966667     0.826876       1\n",
       "8488   lavinia     vinne       0.838333     0.716403       1\n",
       "8489   lavinia    wyncha       0.000000     0.197308       1\n",
       "25631  lavinia     jonna       0.033333     0.073134       0\n",
       "55876  lavinia  patricia       0.071667     0.195781       0\n",
       "69592  lavinia     ditus       0.000000     0.101449       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_training[meta_training.name_a=='lavinia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d22ecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute '_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhmni\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m matcher \u001b[38;5;241m=\u001b[39m \u001b[43mhmni\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\hmni\\matcher.py:103\u001b[0m, in \u001b[0;36mMatcher.__init__\u001b[1;34m(self, model, prefilter, allow_alt_surname, allow_initials, allow_missing_components)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterativesubstring\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbisim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscountedlevenshtein\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlcsstr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlipns\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     99\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrcmp95\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmra\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meditex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflexmetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjaro\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigueramico\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    100\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msift4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meudex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcovington\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphoneticeditdistance\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# String Distance Pipeline (Level 0/Base Model)\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbaseModel \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Character Embedding Network (Level 0/Base Model)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m preprocess\u001b[38;5;241m.\u001b[39mVocabularyProcessor(max_document_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, min_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrestore(\n\u001b[0;32m    107\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\joblib\\numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 587\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\joblib\\numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    504\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 506\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    508\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    512\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\pickle.py:1528\u001b[0m, in \u001b[0;36m_Unpickler.load_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1526\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1527\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1528\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(klass)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\pickle.py:1579\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1577\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING:\n\u001b[0;32m   1578\u001b[0m         module \u001b[38;5;241m=\u001b[39m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[1;32m-> 1579\u001b[0m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m   1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\sklearn\\__init__.py:80\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     83\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcovariance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross_decomposition\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     84\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecomposition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdummy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexceptions\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     85\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperimental\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternals\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_extraction\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_config\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_config\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_context\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     95\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\sklearn\\base.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\sklearn\\utils\\__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmurmurhash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m murmurhash3_32\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\sklearn\\utils\\class_weight.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Authors: Andreas Mueller\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#          Manoj Kumar\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# License: BSD 3 clause\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_positional_args\n\u001b[0;32m     10\u001b[0m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_class_weight\u001b[39m(class_weight, \u001b[38;5;241m*\u001b[39m, classes, y):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124;03m\"\"\"Estimate class weights for unbalanced datasets.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    Logistic Regression in Rare Events Data, King, Zen, 2001.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\sklearn\\utils\\validation.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _object_dtype_isnan\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NonBLASDotWarning, PositiveSpectrumWarning\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\sklearn\\utils\\fixes.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lsqr \u001b[38;5;28;01mas\u001b[39;00m sparse_lsqr  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskedArray \u001b[38;5;28;01mas\u001b[39;00m _MaskedArray  \u001b[38;5;66;03m# TODO: remove in 0.25\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\stats\\__init__.py:441\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    438\u001b[0m \n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorestats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\stats\\stats.py:43\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mstats_basic\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_mstats_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     46\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\stats\\distributions.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distn_infrastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (comb, chndtr, entr, xlogy, ive)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# for root finding for continuous distribution ppf, and max likelihood\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# estimation\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m integrate\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\optimize\\__init__.py:400\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=====================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mOptimization and root finding (:mod:`scipy.optimize`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m:mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_minimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_root\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\optimize\\optimize.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinesearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (line_search_wolfe1, line_search_wolfe2,\n\u001b[0;32m     34\u001b[0m                          line_search_wolfe2 \u001b[38;5;28;01mas\u001b[39;00m line_search,\n\u001b[0;32m     35\u001b[0m                          LineSearchWarning)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_numdiff\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m approx_derivative\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getfullargspec_no_self \u001b[38;5;28;01mas\u001b[39;00m _getfullargspec\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MapWrapper\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse, csc_matrix, csr_matrix, coo_matrix, find\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_group_columns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m group_dense, group_sparse\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py:111\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mSparse linear algebra (:mod:`scipy.sparse.linalg`)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m==================================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01misolve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsolve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\sparse\\linalg\\isolve\\__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterative Solvers for Sparse Linear Systems\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#from info import __doc__\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterative\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminres\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minres\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlgmres\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lgmres\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\sparse\\linalg\\isolve\\iterative.py:150\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combine\n\u001b[0;32m    128\u001b[0m \u001b[38;5;129;43m@set_docstring\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUse BIConjugate Gradient iteration to solve ``Ax = b``.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mThe real or complex N-by-N matrix of the linear system.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    130\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAlternatively, ``A`` can be a linear operator which can\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    131\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduce ``Ax`` and ``A^T x`` using, e.g.,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    132\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m``scipy.sparse.linalg.LinearOperator``.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m               \u001b[49m\u001b[43mfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;43m               \u001b[39;49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;43m               Examples\u001b[39;49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;43m               --------\u001b[39;49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;43m               >>> from scipy.sparse import csc_matrix\u001b[39;49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;43m               >>> from scipy.sparse.linalg import bicg\u001b[39;49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;43m               >>> A = csc_matrix([[3, 2, 0], [1, -1, 0], [0, 5, 1]], dtype=float)\u001b[39;49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;43m               >>> b = np.array([2, 4, -1], dtype=float)\u001b[39;49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;43m               >>> x, exitCode = bicg(A, b)\u001b[39;49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;43m               >>> print(exitCode)            # 0 indicates successful convergence\u001b[39;49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;43m               0\u001b[39;49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;43m               >>> np.allclose(A.dot(x), b)\u001b[39;49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;43m               True\u001b[39;49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;43m               \u001b[39;49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;43m               \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[0;32m    148\u001b[0m \u001b[43m               \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;129;43m@non_reentrant\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mbicg\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmake_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\_lib\\_threadsafety.py:57\u001b[0m, in \u001b[0;36mnon_reentrant.<locals>.decorator\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m     55\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not re-entrant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     56\u001b[0m lock \u001b[38;5;241m=\u001b[39m ReentrancyLock(msg)\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecorate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Springboard\\lib\\site-packages\\scipy\\_lib\\_threadsafety.py:45\u001b[0m, in \u001b[0;36mReentrancyLock.decorate\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lib\u001b[49m\u001b[38;5;241m.\u001b[39mdecorator\u001b[38;5;241m.\u001b[39mdecorate(func, caller)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy' has no attribute '_lib'"
     ]
    }
   ],
   "source": [
    "import hmni\n",
    "matcher = hmni.Matcher(model='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df1f9d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher.similarity('kizza', 'wally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913b5e83",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MinMaxScaler' object has no attribute 'clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlavinia\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlavina\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hmni\\matcher.py:266\u001b[0m, in \u001b[0;36mMatcher.similarity\u001b[1;34m(self, name_a, name_b, prob, threshold, surname_first)\u001b[0m\n\u001b[0;32m    264\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeaturize(pair)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# make inference on meta model\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_inf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_component:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# add pair score to the seen dictionary\u001b[39;00m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_pairs[\u001b[38;5;28mhash\u001b[39m(pair)] \u001b[38;5;241m=\u001b[39m sim\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hmni\\matcher.py:418\u001b[0m, in \u001b[0;36mMatcher.meta_inf\u001b[1;34m(self, pair, base_features)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_inf\u001b[39m(\u001b[38;5;28mself\u001b[39m, pair, base_features):\n\u001b[0;32m    417\u001b[0m     meta_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m--> 418\u001b[0m     meta_features[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_inf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m     meta_features[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msiamese_inf(pair)\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;66;03m# add base features to meta_features ('tkn_set', 'iterativesubstring', 'strcmp95')\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hmni\\matcher.py:413\u001b[0m, in \u001b[0;36mMatcher.base_model_inf\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbase_model_inf\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;66;03m# get the positive class prediction from model\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaseModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:523\u001b[0m, in \u001b[0;36mPipeline.predict_proba\u001b[1;34m(self, X, **predict_proba_params)\u001b[0m\n\u001b[0;32m    521\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 523\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict_proba(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_proba_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:509\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    507\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    508\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m:\n\u001b[0;32m    510\u001b[0m     np\u001b[38;5;241m.\u001b[39mclip(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_range[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_range[\u001b[38;5;241m1\u001b[39m], out\u001b[38;5;241m=\u001b[39mX)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MinMaxScaler' object has no attribute 'clip'"
     ]
    }
   ],
   "source": [
    "matcher.similarity('lavinia', 'lavina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "979a790f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MinMaxScaler' object has no attribute 'clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbourque\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbork\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hmni\\matcher.py:266\u001b[0m, in \u001b[0;36mMatcher.similarity\u001b[1;34m(self, name_a, name_b, prob, threshold, surname_first)\u001b[0m\n\u001b[0;32m    264\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeaturize(pair)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# make inference on meta model\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_inf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_component:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# add pair score to the seen dictionary\u001b[39;00m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen_pairs[\u001b[38;5;28mhash\u001b[39m(pair)] \u001b[38;5;241m=\u001b[39m sim\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hmni\\matcher.py:418\u001b[0m, in \u001b[0;36mMatcher.meta_inf\u001b[1;34m(self, pair, base_features)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_inf\u001b[39m(\u001b[38;5;28mself\u001b[39m, pair, base_features):\n\u001b[0;32m    417\u001b[0m     meta_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m--> 418\u001b[0m     meta_features[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_inf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m     meta_features[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msiamese_inf(pair)\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;66;03m# add base features to meta_features ('tkn_set', 'iterativesubstring', 'strcmp95')\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hmni\\matcher.py:413\u001b[0m, in \u001b[0;36mMatcher.base_model_inf\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbase_model_inf\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;66;03m# get the positive class prediction from model\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaseModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:523\u001b[0m, in \u001b[0;36mPipeline.predict_proba\u001b[1;34m(self, X, **predict_proba_params)\u001b[0m\n\u001b[0;32m    521\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 523\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict_proba(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_proba_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:509\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    507\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    508\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m:\n\u001b[0;32m    510\u001b[0m     np\u001b[38;5;241m.\u001b[39mclip(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_range[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_range[\u001b[38;5;241m1\u001b[39m], out\u001b[38;5;241m=\u001b[39mX)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MinMaxScaler' object has no attribute 'clip'"
     ]
    }
   ],
   "source": [
    "matcher.similarity('bourque', 'bork')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
