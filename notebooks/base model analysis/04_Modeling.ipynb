{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ad0746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 16:26:08.198130: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 16:26:08.528857: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-23 16:26:08.528924: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-23 16:26:08.568809: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-23 16:26:09.966083: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 16:26:09.966258: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 16:26:09.966274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf \n",
    "import random\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "import unidecode\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "from fuzzywuzzy import fuzz\n",
    "# from tensorflow.python.platform import gfile\n",
    "# from tensorflow.io import gfile\n",
    "import tensorflow.compat.v1.gfile as gfile\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from abydos.phones import *\n",
    "from abydos.distance import (IterativeSubString, BISIM, DiscountedLevenshtein, Prefix, LCSstr, MLIPNS, Strcmp95,\n",
    "MRA, Editex, SAPS, FlexMetric, JaroWinkler, HigueraMico, Sift4, Eudex, ALINE, Covington, PhoneticEditDistance)\n",
    "from abydos.phonetic import PSHPSoundexFirst, Ainsworth\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a154250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84edf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interim Data File Locations\n",
    "interim_data = '../data/interim/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05000356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87550, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(interim_data + 'feature_engineering_results.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b085e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>target</th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>partial</th>\n",
       "      <th>tkn_sort</th>\n",
       "      <th>tkn_set</th>\n",
       "      <th>sum_ipa</th>\n",
       "      <th>pshp_soundex_first</th>\n",
       "      <th>...</th>\n",
       "      <th>editex</th>\n",
       "      <th>saps</th>\n",
       "      <th>flexmetric</th>\n",
       "      <th>jaro</th>\n",
       "      <th>higueramico</th>\n",
       "      <th>sift4</th>\n",
       "      <th>eudex</th>\n",
       "      <th>aline</th>\n",
       "      <th>covington</th>\n",
       "      <th>phoneticeditdistance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13290</th>\n",
       "      <td>Petka</td>\n",
       "      <td>Petenka</td>\n",
       "      <td>1</td>\n",
       "      <td>petka</td>\n",
       "      <td>petenka</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>0.809677</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.792857</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66542</th>\n",
       "      <td>Beede</td>\n",
       "      <td>Donny</td>\n",
       "      <td>0</td>\n",
       "      <td>beede</td>\n",
       "      <td>donny</td>\n",
       "      <td>62</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.881720</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.872059</td>\n",
       "      <td>0.529630</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80128</th>\n",
       "      <td>Vili</td>\n",
       "      <td>Tamzine</td>\n",
       "      <td>0</td>\n",
       "      <td>vili</td>\n",
       "      <td>tamzine</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0.721774</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307143</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.061905</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.235135</td>\n",
       "      <td>0.462264</td>\n",
       "      <td>0.529954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11664</th>\n",
       "      <td>Miodrag</td>\n",
       "      <td>Mil</td>\n",
       "      <td>1</td>\n",
       "      <td>miodrag</td>\n",
       "      <td>mil</td>\n",
       "      <td>71</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>0.366359</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378571</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.097619</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.984804</td>\n",
       "      <td>0.310811</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.414747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46639</th>\n",
       "      <td>Josefina</td>\n",
       "      <td>Valentina</td>\n",
       "      <td>0</td>\n",
       "      <td>josefina</td>\n",
       "      <td>valentina</td>\n",
       "      <td>79</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>0.740143</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.522222</td>\n",
       "      <td>0.564815</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.519149</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.793907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18767</th>\n",
       "      <td>Styopanka</td>\n",
       "      <td>Dunyasha</td>\n",
       "      <td>0</td>\n",
       "      <td>styopanka</td>\n",
       "      <td>dunyasha</td>\n",
       "      <td>65</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>0.630824</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.699020</td>\n",
       "      <td>0.378723</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.740143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10779</th>\n",
       "      <td>Maure</td>\n",
       "      <td>More</td>\n",
       "      <td>1</td>\n",
       "      <td>maure</td>\n",
       "      <td>more</td>\n",
       "      <td>83</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>0.978495</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.793548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7099</th>\n",
       "      <td>Joakim</td>\n",
       "      <td>Jocke</td>\n",
       "      <td>1</td>\n",
       "      <td>joakim</td>\n",
       "      <td>jocke</td>\n",
       "      <td>77</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>0.576037</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.994608</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.663636</td>\n",
       "      <td>0.752688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77961</th>\n",
       "      <td>Henrietta</td>\n",
       "      <td>Virgil</td>\n",
       "      <td>0</td>\n",
       "      <td>henrietta</td>\n",
       "      <td>virgil</td>\n",
       "      <td>57</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.487097</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.121032</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.180851</td>\n",
       "      <td>0.458904</td>\n",
       "      <td>0.600358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>Gustav</td>\n",
       "      <td>Les</td>\n",
       "      <td>0</td>\n",
       "      <td>gustav</td>\n",
       "      <td>les</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.276498</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.786765</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.427419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               a          b  target     name_a     name_b  partial  tkn_sort  \\\n",
       "13290      Petka    Petenka       1      petka    petenka       77        80   \n",
       "66542      Beede      Donny       0      beede      donny       62        17   \n",
       "80128       Vili    Tamzine       0       vili    tamzine       67        14   \n",
       "11664    Miodrag        Mil       1    miodrag        mil       71        36   \n",
       "46639   Josefina  Valentina       0   josefina  valentina       79        35   \n",
       "18767  Styopanka   Dunyasha       0  styopanka   dunyasha       65        38   \n",
       "10779      Maure       More       1      maure       more       83        73   \n",
       "7099      Joakim      Jocke       1     joakim      jocke       77        62   \n",
       "77961  Henrietta     Virgil       0  henrietta     virgil       57        22   \n",
       "19999     Gustav        Les       0     gustav        les       57        20   \n",
       "\n",
       "       tkn_set   sum_ipa  pshp_soundex_first  ...    editex      saps  \\\n",
       "13290       80  0.809677                   1  ...  0.714286  0.409091   \n",
       "66542       17  0.881720                   0  ...  0.400000  0.000000   \n",
       "80128       14  0.721774                   0  ...  0.214286  0.000000   \n",
       "11664       36  0.366359                   0  ...  0.428571  0.000000   \n",
       "46639       43  0.740143                   0  ...  0.500000  0.034483   \n",
       "18767       38  0.630824                   0  ...  0.333333  0.000000   \n",
       "10779       73  0.978495                   1  ...  0.800000  0.733333   \n",
       "7099        62  0.576037                   1  ...  0.583333  0.625000   \n",
       "77961       22  0.487097                   0  ...  0.333333  0.000000   \n",
       "19999       20  0.276498                   0  ...  0.250000  0.000000   \n",
       "\n",
       "       flexmetric      jaro  higueramico     sift4     eudex     aline  \\\n",
       "13290    0.792857  0.904762     0.690476  0.714286  0.960784  0.621622   \n",
       "66542    0.380000  0.000000     0.005952  0.200000  0.872059  0.529630   \n",
       "80128    0.307143  0.464286     0.061905  0.142857  0.725000  0.235135   \n",
       "11664    0.378571  0.650794     0.097619  0.285714  0.984804  0.310811   \n",
       "46639    0.522222  0.564815     0.444444  0.444444  0.849020  0.519149   \n",
       "18767    0.333333  0.569444     0.288889  0.333333  0.699020  0.378723   \n",
       "10779    0.640000  0.783333     0.600000  0.600000  1.000000  0.847826   \n",
       "7099     0.516667  0.700000     0.500000  0.500000  0.994608  0.583333   \n",
       "77961    0.411111  0.351852     0.121032  0.222222  0.843137  0.180851   \n",
       "19999    0.283333  0.500000     0.050000  0.166667  0.786765  0.220588   \n",
       "\n",
       "       covington  phoneticeditdistance  \n",
       "13290   0.830508              0.714286  \n",
       "66542   0.462963              0.822581  \n",
       "80128   0.462264              0.529954  \n",
       "11664   0.500000              0.414747  \n",
       "46639   0.676471              0.793907  \n",
       "18767   0.541176              0.740143  \n",
       "10779   0.811111              0.793548  \n",
       "7099    0.663636              0.752688  \n",
       "77961   0.458904              0.600358  \n",
       "19999   0.488372              0.427419  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb691ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING TESTING TESTING\n",
    "df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e19c0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e11feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Set:  (100, 27)\n",
      "Label Set:  (100,)\n"
     ]
    }
   ],
   "source": [
    "y = df.target\n",
    "X = df.drop('target', axis=1)\n",
    "print(\"Feature Set: \", X.shape)\n",
    "print(\"Label Set: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f32448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Feature Set:  (80, 27)\n",
      "Training Label Set:  (80,)\n",
      "Testing Feature Set:  (20, 27)\n",
      "Testing Label Set:  (20,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "print(\"Training Feature Set: \", X_train.shape)\n",
    "print(\"Training Label Set: \", y_train.shape)\n",
    "print(\"Testing Feature Set: \", X_test.shape)\n",
    "print(\"Testing Label Set: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a195e9",
   "metadata": {},
   "source": [
    "### Base-Model 1: Exported TPOT Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53dfd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_1(X_train, y_train, X_test, export=False):\n",
    "    exported_pipeline = make_pipeline(\n",
    "        MaxAbsScaler(),\n",
    "        MinMaxScaler(),\n",
    "        RandomForestClassifier(\n",
    "            bootstrap=False,\n",
    "            criterion=\"gini\",\n",
    "            max_features=0.25,\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=4,\n",
    "            n_estimators=100)\n",
    "    )\n",
    "    exported_pipeline.fit(X_train, y_train)\n",
    "    if export==True:\n",
    "        return exported_pipeline\n",
    "    else:\n",
    "        y_pred = exported_pipeline.predict_proba(X_test)\n",
    "        return [p[1] for p in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382027c",
   "metadata": {},
   "source": [
    "### Base-Model 2: Deep LSTM Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ddc5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow based implementation of deep siamese LSTM network.\n",
    "# Taken from https://github.com/dhwajraj/deep-siamese-text-similarity as of 2020-07-20\n",
    "# and modified to fit hmni prediction pipeline\n",
    "# deep-siamese-text-similarity original copyright:\n",
    "#\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2016 Dhwaj Raj\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SiameseLSTM(object):\n",
    "    \"\"\"\n",
    "    A LSTM based deep Siamese network for text similarity.\n",
    "    Uses an character embedding layer, followed by a biLSTM and Energy Loss layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def BiRNN(self, x, dropout, scope, hidden_units):\n",
    "        n_hidden = hidden_units\n",
    "        n_layers = 3\n",
    "\n",
    "        # Prepare data shape to match `static_rnn` function requirements\n",
    "        x = tf.unstack(tf.transpose(x, perm=[1, 0, 2]))\n",
    "\n",
    "        # Define lstm cells with tensorflow\n",
    "        # Forward direction cell\n",
    "        with tf.name_scope('fw' + scope):\n",
    "            with tf.compat.v1.variable_scope('fw' + scope):\n",
    "                stacked_rnn_fw = []\n",
    "                for _ in range(n_layers):\n",
    "                    fw_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "                    lstm_fw_cell = \\\n",
    "                        tf.compat.v1.nn.rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=dropout)\n",
    "                    stacked_rnn_fw.append(lstm_fw_cell)\n",
    "                lstm_fw_cell_m = \\\n",
    "                    tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "\n",
    "        with tf.name_scope('bw' + scope):\n",
    "            with tf.compat.v1.variable_scope('bw' + scope):\n",
    "                stacked_rnn_bw = []\n",
    "                for _ in range(n_layers):\n",
    "                    bw_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "                    lstm_bw_cell = \\\n",
    "                        tf.compat.v1.nn.rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=dropout)\n",
    "                    stacked_rnn_bw.append(lstm_bw_cell)\n",
    "                lstm_bw_cell_m = \\\n",
    "                    tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n",
    "\n",
    "        # Get lstm cell output\n",
    "        with tf.name_scope('bw' + scope):\n",
    "            with tf.compat.v1.variable_scope('bw' + scope):\n",
    "                (outputs, _, _) = \\\n",
    "                    tf.compat.v1.nn.static_bidirectional_rnn(lstm_fw_cell_m,\n",
    "                                                   lstm_bw_cell_m, x, dtype=tf.float32)\n",
    "        return outputs[-1]\n",
    "\n",
    "    def contrastive_loss(self, y, d, batch_size):\n",
    "        tmp = y * tf.square(d)\n",
    "        tmp2 = (1 - y) * tf.square(tf.maximum(1 - d, 0))\n",
    "        return tf.reduce_sum(tmp + tmp2) / batch_size / 2\n",
    "\n",
    "    def __init__(self, sequence_length, vocab_size, embedding_size, hidden_units, batch_size):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.compat.v1.placeholder(tf.int32, [None, sequence_length], name='input_x1')\n",
    "        self.input_x2 = tf.compat.v1.placeholder(tf.int32, [None, sequence_length], name='input_x2')\n",
    "        self.input_y = tf.compat.v1.placeholder(tf.float32, [None], name='input_y')\n",
    "        self.dropout_keep_prob = tf.compat.v1.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.Variable(tf.compat.v1.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                                 trainable=True, name='W')\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        with tf.name_scope('output'):\n",
    "            self.out1 = self.BiRNN(\n",
    "                self.embedded_chars1,\n",
    "                self.dropout_keep_prob,\n",
    "                'side1',\n",
    "                hidden_units\n",
    "            )\n",
    "            self.out2 = self.BiRNN(\n",
    "                self.embedded_chars2,\n",
    "                self.dropout_keep_prob,\n",
    "                'side2',\n",
    "                hidden_units\n",
    "            )\n",
    "            self.distance = \\\n",
    "                tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.out1, self.out2)), 1, keepdims=True))\n",
    "            self.distance = tf.compat.v1.div(self.distance,\n",
    "                                   tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.out1), 1, keepdims=True)),\n",
    "                                          tf.sqrt(tf.reduce_sum(tf.square(self.out2), 1, keepdims=True))))\n",
    "            self.distance = tf.reshape(self.distance, [-1], name='distance')\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = self.contrastive_loss(self.input_y, self.distance, batch_size)\n",
    "\n",
    "        # Accuracy computation is outside of this class.\n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.temp_sim = tf.subtract(tf.ones_like(self.distance),\n",
    "                                        tf.compat.v1.rint(self.distance), name='temp_sim')  # auto threshold 0.5\n",
    "            correct_predictions = tf.equal(self.temp_sim, self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de3794f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow based implementation of deep siamese LSTM network.\n",
    "# Taken from https://github.com/dhwajraj/deep-siamese-text-similarity as of 2020-07-20\n",
    "# and modified to fit hmni prediction pipeline\n",
    "# deep-siamese-text-similarity original copyright:\n",
    "#\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2016 Dhwaj Raj\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "TOKENIZER_RE = re.compile(r\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", re.UNICODE)\n",
    "\n",
    "\n",
    "def tokenizer(iterator):\n",
    "    \"\"\"Tokenizer generator.\n",
    "    Args:\n",
    "      iterator: Input iterator with strings.\n",
    "    Yields:\n",
    "      array of tokens per each value in the input.\n",
    "    \"\"\"\n",
    "    for value in iterator:\n",
    "        yield TOKENIZER_RE.findall(value)\n",
    "\n",
    "\n",
    "class CategoricalVocabulary(object):\n",
    "    \"\"\"Categorical variables vocabulary class.\n",
    "  Accumulates and provides mapping from classes to indexes.\n",
    "  Can be easily used for words.\n",
    "  \"\"\"\n",
    "    def __init__(self, unknown_token=\"<UNK>\", support_reverse=True):\n",
    "        self._unknown_token = unknown_token\n",
    "        self._mapping = {unknown_token: 0}\n",
    "        self._support_reverse = support_reverse\n",
    "        if support_reverse:\n",
    "            self._reverse_mapping = [unknown_token]\n",
    "        self._freq = collections.defaultdict(int)\n",
    "        self._freeze = False\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns total count of mappings. Including unknown token.\"\"\"\n",
    "        return len(self._mapping)\n",
    "\n",
    "    def freeze(self, freeze=True):\n",
    "        \"\"\"Freezes the vocabulary, after which new words return unknown token id.\n",
    "        Args:\n",
    "          freeze: True to freeze, False to unfreeze.\n",
    "        \"\"\"\n",
    "        self._freeze = freeze\n",
    "\n",
    "    def get(self, category):\n",
    "        \"\"\"Returns word's id in the vocabulary.\n",
    "        If category is new, creates a new id for it.\n",
    "        Args:\n",
    "          category: string or integer to lookup in vocabulary.\n",
    "        Returns:\n",
    "          interger, id in the vocabulary.\n",
    "        \"\"\"\n",
    "        if category not in self._mapping:\n",
    "            if self._freeze:\n",
    "                return 0\n",
    "            self._mapping[category] = len(self._mapping)\n",
    "            if self._support_reverse:\n",
    "                self._reverse_mapping.append(category)\n",
    "        return self._mapping[category]\n",
    "\n",
    "    def add(self, category, count=1):\n",
    "        \"\"\"Adds count of the category to the frequency table.\n",
    "        Args:\n",
    "          category: string or integer, category to add frequency to.\n",
    "          count: optional integer, how many to add.\n",
    "        \"\"\"\n",
    "        category_id = self.get(category)\n",
    "        if category_id <= 0:\n",
    "            return\n",
    "        self._freq[category] += count\n",
    "\n",
    "    def trim(self, min_frequency, max_frequency=-1):\n",
    "        \"\"\"Trims vocabulary for minimum frequency.\n",
    "        Remaps ids from 1..n in sort frequency order.\n",
    "        where n - number of elements left.\n",
    "        Args:\n",
    "          min_frequency: minimum frequency to keep.\n",
    "          max_frequency: optional, maximum frequency to keep.\n",
    "            Useful to remove very frequent categories (like stop words).\n",
    "        \"\"\"\n",
    "        # Sort by alphabet then reversed frequency.\n",
    "        self._freq = sorted(\n",
    "            sorted(\n",
    "                self._freq.items(),\n",
    "                key=lambda x: (isinstance(x[0], str), x[0])),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True)\n",
    "        self._mapping = {self._unknown_token: 0}\n",
    "        if self._support_reverse:\n",
    "            self._reverse_mapping = [self._unknown_token]\n",
    "        idx = 1\n",
    "        for category, count in self._freq:\n",
    "            if 0 < max_frequency <= count:\n",
    "                continue\n",
    "            if count <= min_frequency:\n",
    "                break\n",
    "            self._mapping[category] = idx\n",
    "            idx += 1\n",
    "            if self._support_reverse:\n",
    "                self._reverse_mapping.append(category)\n",
    "        self._freq = dict(self._freq[:idx - 1])\n",
    "\n",
    "    def reverse(self, class_id):\n",
    "        \"\"\"Given class id reverse to original class name.\n",
    "        Args:\n",
    "          class_id: Id of the class.\n",
    "        Returns:\n",
    "          Class name.\n",
    "        Raises:\n",
    "          ValueError: if this vocabulary wasn't initialized with support_reverse.\n",
    "        \"\"\"\n",
    "        if not self._support_reverse:\n",
    "            raise ValueError(\"This vocabulary wasn't initialized with \"\n",
    "                             \"support_reverse to support reverse() function.\")\n",
    "        return self._reverse_mapping[class_id]\n",
    "\n",
    "\n",
    "class VocabularyProcessor(object):\n",
    "    \"\"\"Maps documents to sequences of word ids.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_document_length,\n",
    "                 min_frequency=0,\n",
    "                 vocabulary=None,\n",
    "                 tokenizer_fn=None):\n",
    "        \"\"\"Initializes a VocabularyProcessor instance.\n",
    "        Args:\n",
    "          max_document_length: Maximum length of documents.\n",
    "            if documents are longer, they will be trimmed, if shorter - padded.\n",
    "          min_frequency: Minimum frequency of words in the vocabulary.\n",
    "          vocabulary: CategoricalVocabulary object.\n",
    "        Attributes:\n",
    "          vocabulary_: CategoricalVocabulary object.\n",
    "        \"\"\"\n",
    "        self.max_document_length = max_document_length\n",
    "        self.min_frequency = min_frequency\n",
    "        if vocabulary:\n",
    "            self.vocabulary_ = vocabulary\n",
    "        else:\n",
    "            self.vocabulary_ = CategoricalVocabulary()\n",
    "        if tokenizer_fn:\n",
    "            self._tokenizer = tokenizer_fn\n",
    "        else:\n",
    "            self._tokenizer = tokenizer\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        \"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "        Args:\n",
    "          raw_documents: An iterable which yield either str or unicode.\n",
    "        Returns:\n",
    "          self\n",
    "        \"\"\"\n",
    "        for tokens in self._tokenizer(raw_documents):\n",
    "            for token in tokens:\n",
    "                self.vocabulary_.add(token)\n",
    "        if self.min_frequency > 0:\n",
    "            self.vocabulary_.trim(self.min_frequency)\n",
    "        self.vocabulary_.freeze()\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, raw_documents):\n",
    "        \"\"\"Learn the vocabulary dictionary and return indexies of words.\n",
    "        Args:\n",
    "          raw_documents: An iterable which yield either str or unicode.\n",
    "        Returns:\n",
    "          x: iterable, [n_samples, max_document_length]. Word-id matrix.\n",
    "        \"\"\"\n",
    "        self.fit(raw_documents)\n",
    "        return self.transform(raw_documents)\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to word-id matrix.\n",
    "        Convert words to ids with vocabulary fitted with fit or the one\n",
    "        provided in the constructor.\n",
    "        Args:\n",
    "          raw_documents: An iterable which yield either str or unicode.\n",
    "        Yields:\n",
    "          x: iterable, [n_samples, max_document_length]. Word-id matrix.\n",
    "        \"\"\"\n",
    "        for tokens in self._tokenizer(raw_documents):\n",
    "            word_ids = np.zeros(self.max_document_length, np.int64)\n",
    "            for idx, token in enumerate(tokens):\n",
    "                if idx >= self.max_document_length:\n",
    "                    break\n",
    "                word_ids[idx] = self.vocabulary_.get(token)\n",
    "            yield word_ids\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves vocabulary processor into given file.\n",
    "        Args:\n",
    "          filename: Path to output file.\n",
    "        \"\"\"\n",
    "        with gfile.Open(filename, 'wb') as f:\n",
    "            f.write(pickle.dumps(self))\n",
    "\n",
    "    @classmethod\n",
    "    def restore(cls, filename):\n",
    "        \"\"\"Restores vocabulary processor from given file.\n",
    "        Args:\n",
    "          filename: Path to file to load from.\n",
    "        Returns:\n",
    "          VocabularyProcessor object.\n",
    "        \"\"\"\n",
    "        with gfile.Open(filename, 'rb') as f:\n",
    "            return pickle.loads(f.read())\n",
    "\n",
    "\n",
    "def tokenizer_char(iterator):\n",
    "    for value in iterator:\n",
    "        yield list(value)\n",
    "\n",
    "\n",
    "class MyVocabularyProcessor(VocabularyProcessor):\n",
    "    def __init__(self, max_document_length, min_frequency=0, vocabulary=None):\n",
    "        super().__init__(max_document_length, min_frequency, vocabulary)\n",
    "        sup = super(MyVocabularyProcessor, self)\n",
    "        sup.__init__(max_document_length, min_frequency, vocabulary,\n",
    "                     tokenizer_char)\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to word-id matrix.\n",
    "        Convert words to ids with vocabulary fitted with fit or the one\n",
    "        provided in the constructor.\n",
    "        Args:\n",
    "          raw_documents: An iterable which yield either str or unicode.\n",
    "        Yields:\n",
    "          x: iterable, [n_samples, max_document_length]. Word-id matrix.\n",
    "        \"\"\"\n",
    "        for tokens in self._tokenizer(raw_documents):\n",
    "            word_ids = np.zeros(self.max_document_length, np.int64)\n",
    "            for (idx, token) in enumerate(tokens):\n",
    "                if idx >= self.max_document_length:\n",
    "                    break\n",
    "                word_ids[idx] = self.vocabulary_.get(token)\n",
    "            yield word_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3244a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_2(X_train, y_train, X_test, export=False):\n",
    "    \n",
    "    # Train Model\n",
    "    embedding_dim = 300  # Dimensionality of character embedding\n",
    "    dropout_keep_prob = 0.8  # Dropout keep probability\n",
    "    hidden_units = 50\n",
    "    batch_size = 64\n",
    "    num_epochs = 300  # Number of training epochs\n",
    "    evaluate_every = 1000  # Evaluate model on dev set after this many steps\n",
    "    max_document_length = 15\n",
    "    # out_dir = os.getcwd()+'\\\\'  # where to save exported models\n",
    "    out_dir = os.getcwd()  # where to save exported models\n",
    "\n",
    "    inpH = InputHelper()\n",
    "    train_set, dev_set, vocab_processor, sum_no_of_batches = \\\n",
    "        inpH.get_datasets(\n",
    "        X_train[['name_a', 'name_b']],\n",
    "        y_train,\n",
    "        max_document_length=max_document_length,\n",
    "        percent_dev=10,\n",
    "        batch_size=64)\n",
    "\n",
    "\n",
    "    # print('starting graph def')\n",
    "    graph = tf.Graph()\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.compat.v1.Session(config=session_conf)\n",
    "        # print('started session')\n",
    "        with sess.as_default():\n",
    "            siameseModel = SiameseLSTM(\n",
    "                sequence_length=max_document_length,\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=embedding_dim,\n",
    "                hidden_units=hidden_units,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            # optimizer = tf.optimizers.Adam(1e-3)\n",
    "            # optimizer = Adam(1e-3)\n",
    "            optimizer = tf.compat.v1.train.AdamOptimizer(1e-3)\n",
    "            # print('initialized siameseModel object')\n",
    "\n",
    "        grads_and_vars = optimizer.compute_gradients(siameseModel.loss)\n",
    "        tr_op_set = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        # print('defined training_ops')\n",
    "        \n",
    "        if export==True:\n",
    "            saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables(), max_to_keep=100)\n",
    "            # Write vocabulary\n",
    "            # print('out_dir: ', out_dir)\n",
    "            # print('os.path: ', os.path.join(out_dir, 'vocab'))\n",
    "            vocab_processor.save(os.path.join(out_dir, 'vocab'))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        def train_step(x1_batch, x2_batch, y_batch):\n",
    "            # A single training step\n",
    "            if random.random() > 0.5:\n",
    "                feed_dict = {\n",
    "                    siameseModel.input_x1: x1_batch,\n",
    "                    siameseModel.input_x2: x2_batch,\n",
    "                    siameseModel.input_y: y_batch,\n",
    "                    siameseModel.dropout_keep_prob: dropout_keep_prob,\n",
    "                }\n",
    "            else:\n",
    "                feed_dict = {\n",
    "                    siameseModel.input_x1: x2_batch,\n",
    "                    siameseModel.input_x2: x1_batch,\n",
    "                    siameseModel.input_y: y_batch,\n",
    "                    siameseModel.dropout_keep_prob: dropout_keep_prob,\n",
    "                }\n",
    "            (_, step, loss, accuracy, dist, sim) = \\\n",
    "                sess.run([tr_op_set, global_step, siameseModel.loss, siameseModel.accuracy,\n",
    "                          siameseModel.distance, siameseModel.temp_sim], feed_dict)\n",
    "\n",
    "        def dev_step(x1_batch, x2_batch, y_batch):\n",
    "            # A single training step\n",
    "            if random.random() > 0.5:\n",
    "                feed_dict = {\n",
    "                    siameseModel.input_x1: x1_batch,\n",
    "                    siameseModel.input_x2: x2_batch,\n",
    "                    siameseModel.input_y: y_batch,\n",
    "                    siameseModel.dropout_keep_prob: 1.0,\n",
    "                }\n",
    "            else:\n",
    "                feed_dict = {\n",
    "                    siameseModel.input_x1: x2_batch,\n",
    "                    siameseModel.input_x2: x1_batch,\n",
    "                    siameseModel.input_y: y_batch,\n",
    "                    siameseModel.dropout_keep_prob: 1.0,\n",
    "                }\n",
    "            (step, loss, accuracy, sim) = \\\n",
    "                sess.run([global_step, siameseModel.loss, siameseModel.accuracy,\n",
    "                          siameseModel.temp_sim], feed_dict)\n",
    "            return accuracy\n",
    "\n",
    "        # Generate batches\n",
    "        batches = inpH.batch_iter(list(zip(train_set[0], train_set[1],\n",
    "                                           train_set[2])), batch_size, num_epochs)\n",
    "        max_validation_acc = 0.0\n",
    "        for nn in range(sum_no_of_batches * num_epochs):\n",
    "            batch = next(batches)\n",
    "            if len(batch) < 1:\n",
    "                continue\n",
    "            (x1_batch, x2_batch, y_batch) = zip(*batch)\n",
    "            if len(y_batch) < 1:\n",
    "                continue\n",
    "            train_step(x1_batch, x2_batch, y_batch)\n",
    "            current_step = tf.compat.v1.train.global_step(sess, global_step)\n",
    "            sum_acc = 0.0\n",
    "            if current_step % evaluate_every == 0:\n",
    "                dev_batches = inpH.batch_iter(list(zip(dev_set[0], dev_set[1], dev_set[2])), batch_size, 1)\n",
    "                for db in dev_batches:\n",
    "                    if len(db) < 1:\n",
    "                        continue\n",
    "                    (x1_dev_b, x2_dev_b, y_dev_b) = zip(*db)\n",
    "                    if len(y_dev_b) < 1:\n",
    "                        continue\n",
    "                    acc = dev_step(x1_dev_b, x2_dev_b, y_dev_b)\n",
    "                    sum_acc = sum_acc + acc\n",
    "            if sum_acc > max_validation_acc:\n",
    "                max_validation_acc = sum_acc\n",
    "            \n",
    "                if export==True:\n",
    "                    # save model\n",
    "                    saver.save(sess, out_dir, global_step=current_step)\n",
    "                    tf.train.write_graph(sess.graph.as_graph_def(), out_dir, 'siamese_network.pb', as_text=False)\n",
    "                \n",
    "                # print('model {} with sum_accuracy={}'.format(nn, max_validation_acc))     \n",
    "        if export==True:\n",
    "            return\n",
    "        \n",
    "        # RUN OOF INFERENCE\n",
    "        x1_temp= np.asarray(X_test['name_a'].tolist())\n",
    "        x2_temp= np.asarray(X_test['name_b'].tolist())\n",
    "        \n",
    "        x1 = np.asarray(list(vocab_processor.transform(x1_temp)))\n",
    "        x2 = np.asarray(list(vocab_processor.transform(x2_temp)))\n",
    "\n",
    "        (predictions, sim) = sess.run([siameseModel.distance, siameseModel.temp_sim], {\n",
    "                siameseModel.input_x1: x1,\n",
    "                siameseModel.input_x2: x2,\n",
    "                siameseModel.dropout_keep_prob: 1.0,\n",
    "            })\n",
    "                \n",
    "        sim = predictions.tolist()\n",
    "        sim = [1-x for x in sim]\n",
    "        # print(sim)\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7d2a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow based implementation of deep siamese LSTM network.\n",
    "# Taken from https://github.com/dhwajraj/deep-siamese-text-similarity as of 2020-07-20\n",
    "# and modified to fit hmni prediction pipeline\n",
    "# deep-siamese-text-similarity original copyright:\n",
    "#\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2016 Dhwaj Raj\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "class InputHelper(object):\n",
    "    vocab_processor = None\n",
    "\n",
    "    def batch_iter(\n",
    "            self,\n",
    "            data,\n",
    "            batch_size,\n",
    "            num_epochs,\n",
    "            shuffle=True,\n",
    "    ):\n",
    "\n",
    "        # Generates a batch iterator for a dataset.\n",
    "        data = np.asarray(data)\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int(len(data) / batch_size) + 1\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = \\\n",
    "                    np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                yield shuffled_data[start_index:end_index]\n",
    "\n",
    "    # Data Preparation\n",
    "    def get_datasets(\n",
    "            self,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            max_document_length,\n",
    "            percent_dev,\n",
    "            batch_size,\n",
    "    ):\n",
    "        (x1_text, x2_text, y) = \\\n",
    "            np.asarray(X_train.iloc[:, 0].str.lower()), np.asarray(X_train.iloc[:, 1].str.lower()), np.asarray(y_train)\n",
    "\n",
    "        # Build vocabulary\n",
    "        # print('Building vocabulary')\n",
    "        vocab_processor = MyVocabularyProcessor(max_document_length, min_frequency=0)\n",
    "        vocab_processor.fit_transform(np.concatenate((x2_text, x1_text), axis=0))\n",
    "        # print('Length of loaded vocabulary ={}'.format(len(vocab_processor.vocabulary_)))\n",
    "\n",
    "        sum_no_of_batches = 0\n",
    "        x1 = np.asarray(list(vocab_processor.transform(x1_text)))\n",
    "        x2 = np.asarray(list(vocab_processor.transform(x2_text)))\n",
    "\n",
    "        # Randomly shuffle data\n",
    "        np.random.seed(131)\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "        x1_shuffled = x1[shuffle_indices]\n",
    "        x2_shuffled = x2[shuffle_indices]\n",
    "        y_shuffled = y[shuffle_indices]\n",
    "        dev_idx = -1 * len(y_shuffled) * percent_dev // 100\n",
    "        del x1\n",
    "        del x2\n",
    "\n",
    "        # TODO: This is very crude, should use cross-validation\n",
    "        (x1_train, x1_dev) = (x1_shuffled[:dev_idx], x1_shuffled[dev_idx:])\n",
    "        (x2_train, x2_dev) = (x2_shuffled[:dev_idx], x2_shuffled[dev_idx:])\n",
    "        (y_train, y_dev) = (y_shuffled[:dev_idx], y_shuffled[dev_idx:])\n",
    "        # print('Train/Dev split for data: {:d}/{:d}'.format(len(y_train), len(y_dev)))\n",
    "\n",
    "        sum_no_of_batches = sum_no_of_batches + len(y_train) // batch_size\n",
    "        train_set = (x1_train, x2_train, y_train)\n",
    "        dev_set = (x1_dev, x2_dev, y_dev)\n",
    "        gc.collect()\n",
    "        return train_set, dev_set, vocab_processor, sum_no_of_batches\n",
    "\n",
    "    def getTestDataSet(\n",
    "            self,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            vocab,\n",
    "            max_document_length,\n",
    "    ):\n",
    "        (x1_temp, x2_temp, y) = np.asarray(X_test.iloc[:, 0].str.lower()), np.asarray(\n",
    "            X_test.iloc[:, 1].str.lower()), np.asarray(y_test)\n",
    "\n",
    "        # Build vocabulary\n",
    "        vocab_processor = MyVocabularyProcessor(max_document_length, min_frequency=0)\n",
    "        vocab_processor = vocab\n",
    "\n",
    "        x1 = np.asarray(list(vocab_processor.transform(x1_temp)))\n",
    "        x2 = np.asarray(list(vocab_processor.transform(x2_temp)))\n",
    "\n",
    "        # Randomly shuffle data\n",
    "        del vocab_processor\n",
    "        gc.collect()\n",
    "        return x1, x2, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9422f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26498/1653033814.py:74: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:1654: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/layers/rnn/legacy_cells.py:1048: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 16:26:12.115256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 16:26:12.117052: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-23 16:26:12.117093: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-23 16:26:12.117119: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (michael-notebook): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1176: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 16:26:23.055324: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed fold 1 of 10\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "completed fold 2 of 10\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "completed fold 3 of 10\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "completed fold 4 of 10\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "completed fold 5 of 10\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "completed fold 6 of 10\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "completed fold 7 of 10\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "completed fold 8 of 10\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "completed fold 9 of 10\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "completed fold 10 of 10\n"
     ]
    }
   ],
   "source": [
    "# Stratified K-Folds cross-validator\n",
    "meta_training = pd.DataFrame()\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    oof_pred = X_test[['name_a', 'name_b']]\n",
    "    \n",
    "    oof_pred['predict_proba'] = base_model_1(X_train.drop(['a', 'b', 'name_a', 'name_b'], 1),\n",
    "                                      y_train,\n",
    "                                      X_test.drop(['a', 'b', 'name_a', 'name_b'], 1))\n",
    "\n",
    "    oof_pred['siamese_sim'] = base_model_2(X_train[['name_a', 'name_b']],\n",
    "                                      y_train,\n",
    "                                      X_test[['name_a', 'name_b']])\n",
    "    \n",
    "    oof_pred['target'] = y_test.tolist()\n",
    "    \n",
    "    print('completed fold {} of 10'.format(fold))\n",
    "    fold += 1\n",
    "\n",
    "    meta_training = meta_training.append(oof_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b85afd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>predict_proba</th>\n",
       "      <th>siamese_sim</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10084</th>\n",
       "      <td>margaret</td>\n",
       "      <td>mae</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.041273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63002</th>\n",
       "      <td>demaris</td>\n",
       "      <td>lerka</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.016828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75871</th>\n",
       "      <td>yisrael</td>\n",
       "      <td>katy</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80763</th>\n",
       "      <td>gonca</td>\n",
       "      <td>orre</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71502</th>\n",
       "      <td>vicke</td>\n",
       "      <td>gloria</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26190</th>\n",
       "      <td>jerom</td>\n",
       "      <td>cadi</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67245</th>\n",
       "      <td>lubomil</td>\n",
       "      <td>demi</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73143</th>\n",
       "      <td>anush</td>\n",
       "      <td>emilunia</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45839</th>\n",
       "      <td>slawomir</td>\n",
       "      <td>livia</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.050592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>jenne</td>\n",
       "      <td>eugenia</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.049211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name_a    name_b  predict_proba  siamese_sim  target\n",
       "10084  margaret       mae       0.686667     0.041273       1\n",
       "63002   demaris     lerka       0.096667     0.016828       0\n",
       "75871   yisrael      katy       0.093333     0.007344       0\n",
       "80763     gonca      orre       0.120000     0.032210       0\n",
       "71502     vicke    gloria       0.000000     0.569258       0\n",
       "26190     jerom      cadi       0.005000     0.002269       0\n",
       "67245   lubomil      demi       0.015000     0.067870       0\n",
       "73143     anush  emilunia       0.000000     0.053945       0\n",
       "45839  slawomir     livia       0.010000     0.050592       0\n",
       "6914      jenne   eugenia       0.013333     0.049211       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_training.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3500b",
   "metadata": {},
   "source": [
    "## Meta-Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "759bc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def syllables(word):\n",
    "    # single syllable word\n",
    "    if len(re.findall('[aeiouy]', word)) <= 1:\n",
    "        return [word]\n",
    "\n",
    "    # sonority hierarchy: vowels, nasals, fricatives, stops\n",
    "    hierarchy = {\n",
    "        'a': 4, 'e': 4, 'i': 4, 'o': 4, 'u': 4, 'y': 4,\n",
    "        'l': 3, 'm': 3, 'n': 3, 'r': 3, 'w': 3,\n",
    "        'f': 2, 's': 2, 'v': 2, 'z': 2,\n",
    "        'b': 1, 'c': 1, 'd': 1, 'g': 1, 'h': 1, 'j': 1, 'k': 1, 'p': 1, 'q': 1, 't': 1, 'x': 1,\n",
    "    }\n",
    "    syllables_values = [(c, hierarchy[c]) for c in word]\n",
    "\n",
    "    syllables = []\n",
    "    syll = syllables_values[0][0]\n",
    "    for trigram in zip(*[syllables_values[i:] for i in range(3)]):\n",
    "        (phonemes, values) = zip(*trigram)\n",
    "        (previous, val, following) = values\n",
    "        phoneme = phonemes[1]\n",
    "\n",
    "        if previous > val < following:\n",
    "            syllables.append(syll)\n",
    "            syll = phoneme\n",
    "        elif previous >= val == following:\n",
    "            syll += phoneme\n",
    "            syllables.append(syll)\n",
    "            syll = ''\n",
    "        else:\n",
    "            syll += phoneme\n",
    "    syll += syllables_values[-1][0]\n",
    "    syllables.append(syll)\n",
    "\n",
    "    final_syllables = []\n",
    "    front = ''\n",
    "    for (i, syllable) in enumerate(syllables):\n",
    "        if not re.search('[aeiouy]', syllable):\n",
    "            if len(final_syllables) == 0:\n",
    "                front += syllable\n",
    "            else:\n",
    "                final_syllables = final_syllables[:-1] \\\n",
    "                                  + [final_syllables[-1] + syllable]\n",
    "        else:\n",
    "            if len(final_syllables) == 0:\n",
    "                final_syllables.append(front + syllable)\n",
    "            else:\n",
    "                final_syllables.append(syllable)\n",
    "    return final_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dc128f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(df):\n",
    "    if len(df.columns)==3:\n",
    "        df.columns=['a', 'b', 'target']\n",
    "    elif len(df.columns)==2:\n",
    "        df.columns=['a', 'b']\n",
    "    else:\n",
    "        df = df.rename(columns={df.columns[0]: 'a', df.columns[1]: 'b' })\n",
    "        \n",
    "    df['name_a'] = df.apply(lambda row: re.sub(\n",
    "        '[^a-zA-Z]+', '', unidecode.unidecode(row['a']).lower().strip()), axis=1)\n",
    "    df['name_b'] = df.apply(lambda row: re.sub(\n",
    "        '[^a-zA-Z]+', '', unidecode.unidecode(row['b']).lower().strip()), axis=1)\n",
    "    \n",
    "    df['syll_a'] = df.apply(lambda row: syllables(row.name_a), axis=1)\n",
    "    df['syll_b'] = df.apply(lambda row: syllables(row.name_b), axis=1)\n",
    "    \n",
    "    df['partial'] = df.apply(lambda row: fuzz.partial_ratio(row.syll_a,row.syll_b), axis=1)\n",
    "    df['tkn_sort'] = df.apply(lambda row: fuzz.token_sort_ratio(row.syll_a,row.syll_b), axis=1)\n",
    "    df['tkn_set'] = df.apply(lambda row: fuzz.token_set_ratio(row.syll_a,row.syll_b), axis=1)\n",
    "    \n",
    "    df['sum_ipa'] = df.apply(lambda row: sum_ipa(row.name_a, row.name_b), axis=1)\n",
    "    \n",
    "    df['pshp_soundex_first'] = df.apply(\n",
    "        lambda row: 1 if pshp_soundex_first.encode(row.name_a)==pshp_soundex_first.encode(row.name_b) else 0, axis=1)\n",
    "    \n",
    "    for i, algo in enumerate(algos):\n",
    "            df[algo_names[i]] = df.apply(lambda row: algo.sim(row.name_a, row.name_b), axis=1)\n",
    "            \n",
    "    df.drop(['syll_a', 'syll_b'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1270efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pshp_soundex_first = PSHPSoundexFirst()\n",
    "pe = Ainsworth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee99dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iss = IterativeSubString()\n",
    "bisim = BISIM()\n",
    "dlev = DiscountedLevenshtein()\n",
    "prefix = Prefix()\n",
    "lcs = LCSstr()\n",
    "mlipns = MLIPNS()\n",
    "strcmp95 = Strcmp95()\n",
    "mra = MRA()\n",
    "editex = Editex()\n",
    "saps = SAPS()\n",
    "flexmetric = FlexMetric()\n",
    "jaro = JaroWinkler(mode='Jaro')\n",
    "higuera_mico = HigueraMico()\n",
    "sift4 = Sift4()\n",
    "eudex = Eudex()\n",
    "aline = ALINE()\n",
    "covington = Covington()\n",
    "phonetic_edit = PhoneticEditDistance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1de1c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [iss, bisim, dlev, prefix, lcs, mlipns, strcmp95, mra, editex, saps, flexmetric, jaro, higuera_mico, sift4, eudex,\n",
    "         aline, covington, phonetic_edit]\n",
    "\n",
    "algo_names = ['iterativesubstring', 'bisim', 'discountedlevenshtein', 'prefix', 'lcsstr', 'mlipns', 'strcmp95', 'mra',\n",
    "              'editex', 'saps', 'flexmetric', 'jaro', 'higueramico', 'sift4', 'eudex', 'aline', 'covington',\n",
    "              'phoneticeditdistance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "644df4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_ipa(name_a, name_b):\n",
    "    feat1 = ipa_to_features(pe.encode(name_a))\n",
    "    feat2 = ipa_to_features(pe.encode(name_b))\n",
    "    score = sum(cmp_features(f1, f2) for f1, f2 in zip(feat1, feat2))/len(feat1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce06301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = featurize(meta_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b048e66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>predict_proba</th>\n",
       "      <th>siamese_sim</th>\n",
       "      <th>target</th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>partial</th>\n",
       "      <th>tkn_sort</th>\n",
       "      <th>tkn_set</th>\n",
       "      <th>...</th>\n",
       "      <th>editex</th>\n",
       "      <th>saps</th>\n",
       "      <th>flexmetric</th>\n",
       "      <th>jaro</th>\n",
       "      <th>higueramico</th>\n",
       "      <th>sift4</th>\n",
       "      <th>eudex</th>\n",
       "      <th>aline</th>\n",
       "      <th>covington</th>\n",
       "      <th>phoneticeditdistance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61490</th>\n",
       "      <td>teodora</td>\n",
       "      <td>nanou</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.202963</td>\n",
       "      <td>0</td>\n",
       "      <td>teodora</td>\n",
       "      <td>nanou</td>\n",
       "      <td>62</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264286</td>\n",
       "      <td>0.447619</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.799020</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>0.645161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>david</td>\n",
       "      <td>dave</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.229473</td>\n",
       "      <td>1</td>\n",
       "      <td>david</td>\n",
       "      <td>dave</td>\n",
       "      <td>83</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.991176</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.793548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68407</th>\n",
       "      <td>agnieszka</td>\n",
       "      <td>xander</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.141512</td>\n",
       "      <td>0</td>\n",
       "      <td>agnieszka</td>\n",
       "      <td>xander</td>\n",
       "      <td>64</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.221032</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.661275</td>\n",
       "      <td>0.321277</td>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.517921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61592</th>\n",
       "      <td>dortha</td>\n",
       "      <td>arju</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.037494</td>\n",
       "      <td>0</td>\n",
       "      <td>dortha</td>\n",
       "      <td>arju</td>\n",
       "      <td>58</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.851471</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.551075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61077</th>\n",
       "      <td>ford</td>\n",
       "      <td>muki</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013328</td>\n",
       "      <td>0</td>\n",
       "      <td>ford</td>\n",
       "      <td>muki</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807353</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.766129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               a       b  predict_proba  siamese_sim  target     name_a  \\\n",
       "61490    teodora   nanou           0.00     0.202963       0    teodora   \n",
       "3198       david    dave           1.00     0.229473       1      david   \n",
       "68407  agnieszka  xander           0.01     0.141512       0  agnieszka   \n",
       "61592     dortha    arju           0.00     0.037494       0     dortha   \n",
       "61077       ford    muki           0.00     0.013328       0       ford   \n",
       "\n",
       "       name_b  partial  tkn_sort  tkn_set  ...    editex      saps  \\\n",
       "61490   nanou       62        40       40  ...  0.357143  0.000000   \n",
       "3198     dave       83        73       73  ...  0.700000  0.733333   \n",
       "68407  xander       64        22       22  ...  0.277778  0.000000   \n",
       "61592    arju       58        33       33  ...  0.333333  0.000000   \n",
       "61077    muki       38         0        0  ...  0.125000  0.000000   \n",
       "\n",
       "       flexmetric      jaro  higueramico     sift4     eudex     aline  \\\n",
       "61490    0.264286  0.447619     0.119048  0.142857  0.799020  0.439394   \n",
       "3198     0.740000  0.783333     0.600000  0.600000  0.991176  0.685185   \n",
       "68407    0.222222  0.611111     0.221032  0.333333  0.661275  0.321277   \n",
       "61592    0.400000  0.472222     0.133333  0.166667  0.851471  0.352941   \n",
       "61077    0.050000  0.000000     0.000000  0.000000  0.807353  0.170833   \n",
       "\n",
       "       covington  phoneticeditdistance  \n",
       "61490   0.516949              0.645161  \n",
       "3198    0.811111              0.793548  \n",
       "68407   0.452055              0.517921  \n",
       "61592   0.551020              0.551075  \n",
       "61077   0.431818              0.766129  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a61dff",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "029bb23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in df.columns if col not in ['a', 'b', 'name_a', 'name_b', 'target', 'predict_proba', 'siamese_sim']]\n",
    "comb2 = list(combinations(cols, 2))\n",
    "comb3 = list(combinations(cols, 3))\n",
    "colgrid = [(col,)for col in cols]+comb2+comb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7ae6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, df.target, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "358e4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='liblinear')\n",
    "grid_clf = GridSearchCV(clf, param_grid = {'C':np.logspace(-4, 4, 20)}, scoring = 'precision', verbose=0)\n",
    "\n",
    "scores = []\n",
    "for cols in colgrid:\n",
    "    grid_clf.fit(X_train[['predict_proba', 'siamese_sim']+list(cols)], y_train)\n",
    "    y_pred = grid_clf.predict(X_val[['predict_proba', 'siamese_sim']+list(cols)])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    scores.append([str(cols), tn, fp, fn, tp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffb2430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d5ee878",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.columns = ['features', 'tn', 'fp', 'fn', 'tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5960d38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('partial',)</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('tkn_sort',)</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('tkn_set',)</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('sum_ipa',)</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('pshp_soundex_first',)</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  features  tn  fp  fn  tp\n",
       "0             ('partial',)  18   0   1   1\n",
       "1            ('tkn_sort',)  18   0   1   1\n",
       "2             ('tkn_set',)  18   0   1   1\n",
       "3             ('sum_ipa',)  18   0   1   1\n",
       "4  ('pshp_soundex_first',)  18   0   2   0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a71571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df['error'] = scores_df['fp'] + scores_df['fn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d7cf01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = scores_df.sort_values(['error', 'fp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f015a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, df.target, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2881de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['predict_proba', 'siamese_sim', 'tkn_set', 'iterativesubstring', 'strcmp95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7635f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='liblinear')\n",
    "grid_clf = GridSearchCV(clf, param_grid = {'C':np.logspace(-4, 4, 20)}, scoring='precision')\n",
    "grid_clf.fit(X_train[selected_cols], y_train)\n",
    "y_pred = grid_clf.predict(X_test[selected_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0ed7bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.23357214690901212}\n"
     ]
    }
   ],
   "source": [
    "print(grid_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e06de3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51c36f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    conf_matrix = pd.DataFrame(data=cm, columns=['Predicted: 0', 'Predicted: 1'], index=['Actual: 0', 'Actual: 1'])\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1156e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        17\n",
      "           1       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.95      0.67      0.72        20\n",
      "weighted avg       0.91      0.90      0.88        20\n",
      "\n",
      "           Predicted: 0  Predicted: 1\n",
      "Actual: 0            17             0\n",
      "Actual: 1             2             1\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fca8231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.6, 0.3, 'AUC=0.882')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(7,7))\n",
    "baseline_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = grid_clf.predict_proba(X_test[selected_cols])\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# calculate scores\n",
    "baseline_auc = roc_auc_score(y_test, baseline_probs)\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "auc = ('AUC=%.3f' % (auc))\n",
    "\n",
    "# calculate roc curves\n",
    "baseline_fpr, baseline_tpr, _ = roc_curve(y_test, baseline_probs)\n",
    "fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "ax.plot(baseline_fpr, baseline_tpr, color='gray')\n",
    "ax.plot(fpr, tpr, marker='.', color='black')\n",
    "\n",
    "# axis labels\n",
    "ax.set_xlabel('False Positive Rate',fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate',fontsize=12)\n",
    "ax.set_title('Receiver Operating Characteristic', fontsize=16)\n",
    "plt.text(.6, .3, auc, fontsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebfafd",
   "metadata": {},
   "source": [
    "## Export Models and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73c49df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2660f161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_1 = base_model_1(X.drop(['a', 'b', 'name_a', 'name_b'], 1), y, X_test=None, export=True)\n",
    "joblib.dump(base_1, filename='base.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16a56040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "base_model_2(X[['name_a', 'name_b']], y, X_test=None, export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de9f1498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid_clf.best_estimator_, filename='meta.pkl')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
